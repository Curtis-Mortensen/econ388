---
title: "Chapter 4 Homework"
Name: "Curtis Mortensen"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(wooldridge)
library(tidyverse)
library(pander)
library(mosaic)
library(foreign)
library(effects)
library(stats)
```

# Homework Chapter 4 {.tabset}

##  Question 1

The following model can be used to study whether campaign expenditures affect election outcomes:

$$ voteA = \beta_0 + \beta_1log(expendA) + \beta_2log(expendB) + \beta_3prtystrA + u $$

where *voteA* is the percentage of the vote received by Candidate A, *expendA* and *expendB* are campaign expenditures by Candidates A and B, and *prtystrA* is a measure of party strength for Candidate A (the percentage of the most recent presidential vote that went to A's party). 


- see pg 110 for taking lsat and gpa out 

#### a) 

What is the interpretation of $$\beta_0$$?

Beta_0 is the amount of votes that Candidate A would get if they spent nothing on their campaign, their opponent spent nothing on their campaign, and their party didn't recieve any votes in the most recent presidential election. Basically, it's the votes that are due to all other factors, likely primarily the personality and platform of Candidate A. 






#### b) 

In terms of the parameters, state the null hypothesis that a 1% increase in A's expenditures is offset by a 1% increase in B's expenditures.


B1 - B2 /= 0




#### c) 

Estimate the given model using the data in VOTE1 and report the results in usual form. Do A's expenditures affect the outcome? What about B's expenditures? Can you use these results to test the hypothesis in part ii?


```{r}
data("vote1", package='wooldridge')
expendA <- na.omit(vote1$expendA)
expendB <- vote1$expendB
prtyA <- vote1$prtystrA
voteA <- vote1$voteA

m1 <- lm(voteA ~ expendA + expendB + prtyA)
m1
```
Yes, A's expendatures have a coefficient of .035, so each 1% increase in spending causes a 3.5% increase in votes for Candidate A. B's expendatures have the opposite response, with a coefficient of -.035. I can use these to confirm the hypothesis in b that a 1% increase in A's spending is offset by a 1% increase in B's spending. 








#### d) 

Estimate a model that directly gives the t statistic for testing the hypothesis in part ii. What do you conclude? Use a two-sided alternative.

```{r}
t.test(vote1$expendA, mu=0, alternative = 'greater')
```


I haven't been able to figure out exactly how to run a t test for 2 variables. I might have been able to with more time, but I haven't so far... 





## Question 2

Use the data in LAWSCH85 for this exercise. 

#### a)

Using the same model as in Problem 4 in Chapter 3, state and test the null hypothesis that the rank of law schools has no ceteris paribus effect on median starting salary. 

In the model,

$$log(salary) = \beta_{0} + \beta_{1}LSAT + \beta_{3}log(libvol) + \beta_{4}log(cost) $$

```{r}
data("lawsch85", package='wooldridge')
attach(lawsch85)

m2 <- lm(salary~LSAT+llibvol+lcost)
m22 <- lm(salary~rank+LSAT+llibvol+lcost)

detach(lawsch85)
summary(m22)
```
The null hypothesis is that B5(rank) /= 0. That isn't held up in the model.






#### b) 

Are features of the incoming  class of students- namely, LSAT and GPA-individually or jointly significant for explaining salary? Be sure to account for the missing data on LSAT and GPA. 



m4 <- lm(salary~LSAT+GPA)

summary(m4)

#check with degrees of freedom

#### c) 

Test whether the size of the entering class (*clsize*) or the size of the faculty (*faculty*) needs to be added to this equation; carry out a single test. Be careful to account for missing data on *clsize* and *faculty*. 


```{r}
attach(lawsch85)
m4 <- lm(salary~LSAT+GPA+clsize+faculty)
detach(lawsch85)
summary(m4)
```

Based on my model, class size could be included in my equation: it does add greater specificity to my model. However, faculty does not. 




#### d) 

What factors might influence the rank of the law school that are not included in the salary regression?

There's plenty of other factors in the dataset, such as no. of faculty, the student-faculty ratio, and we could imagine many others such as the average incoming GPA, the acceptance rate, or even the rank of the law school by budget. 








## Question 3

Use the data in ELEM94_95 to answer this question. The findings can be compared with those in Table 4.1. The dependent variable *lavgsal* is the log of average teacher salary and *bs* is the ratio of average benefits to average salary (by school).

#### a) 

Run the simple regression of *lavgsal* on *bs*. Is the estimated slope statistically different from zero? Is it statistically different from -1? 

```{r}
data("elem94_95", package='wooldridge')
attach(elem94_95)
m3 <- lm(lavgsal~bs)
summary(m3)
detach(elem94_95)
```
Yes, the slope is statistically different from 0, since the slope is -0.795, which is many times greater than the standard error of .15. Given that standard error the slope is slightly statistically different from -1, since the true population slope is unlikely to be -1. 







#### b) 

Add the variables *lenrol* and *lstaff* to the regression from part i. What happens to coefficient on *bs* How does the situation compare with that in Table 4.1?

```{r}
m32 <- lm(elem94_95$lavgsal~elem94_95$bs+elem94_95$lenrol+elem94_95$lstaff)
summary(m32)
```
The coefficient on bs decreases to -0.61, however it has less variance now with a se of .11 The intercept went up by 3, R squared went from 0.015 to 0.48, so it's possible that bs wasn't explaining much of the regression before. 







#### c)

How come the standard error on the *bs* coefficient is smaller in part ii than in part i? Hint: What happens to the error variance versus multicollinearity when *lenrol* and *lstaff* are added?


Now that we have a more exact model some of the variation which was accounted to bs can be explained by variation in the other variables, lenrol and lstaff. There is little multicollinearity because lenrol and lstaff have very different coefficients (what's the technical term for that?)





#### d) 

How come the coefficient on *lstaff* is negative? Is it large in magnitude (econ sig)?

lstaff is the percentage change in the ratio of staff to students. We would expect that as there are more staff per 1000 students the demands on each teacher will go down, and they will be payed a lower salary. lstaff is statistically significant, and economically significant as well, since a 1% change in teacher-student ratio equals a -0.71% change in salary. 








#### e) 

Now add the variable *lunch* to the regression. Holding other factors fixed, are teachers being compensated for teaching students from disadvantaged backgrounds? Explain. 


```{r}
m33 <- lm(formula = elem94_95$lavgsal ~ elem94_95$bs + elem94_95$lenrol + elem94_95$lstaff + elem94_95$lunch)
summary(m33)
```
It would appear that teachers are not being compensated for teaching disadvantaged students, and in fact the opposite is true. We can infer that districts with many disadvantaged students have smaller budgets, and thus teachers are paid less. 






#### f) 

Overall, is the pattern of results that you find with ELEM94_95 consistent with the pattern in Table 4.1 (found in the textbook on page 138)?

So, assuming I got the right table (it was actually on 119 for my digital edition), and assuming I understood the homoskedastic normal distribution with a single explanatory variable correctly, I think my data is different. Specifically, my model has more than one explanatory variable, although I think all the variables are homoskedastic. 






## Question 4

Use the data in HTV to answer this question. 



#### a) 

Estimate the regression model:

$$ educ = \beta_0 + \beta_1motheduc + \beta_2fatheduc + \beta_3abil + b_4abil^2 + u $$

by OLS and report the results in the usual form. Test the null hypothesis that educ is linearly related to abil against the alternative that the relationship is quadratic. 

```{r}
data("htv", package='wooldridge')
attach(htv)
abil2 <- abil^2
m4 <- lm(educ~motheduc+fatheduc+abil2)
m4og <- lm(educ~motheduc+fatheduc+abil)

summary(m4)
summary(m4og)
```

It would appear that the relationship between education and ability is linear as opposed to quadratic. There is a better t value and better r squared for the model that evaluates ability as linear. 




#### b) 

Using the equation in part i, test $$H_0: \beta_1 = \beta_2$$ against a two-sided alternative. What is the p-value of the test?

I'm still having trouble running t tests :(






#### c) 

Add the two college tuition variables to the regression from part i and determine whether they are jointly statistically significant. 


```{r}
data("htv", package='wooldridge')
attach(htv)
tuit <- (tuit17+tuit18)/2
m42 <- lm(educ~motheduc+fatheduc+abil+tuit)
m43 <- lm(educ~motheduc+fatheduc+abil+tuit17+tuit18)

summary(m42)
summary(m43)
```

It does not appear that tuition is jointly significant, assuming that I added it together correctly. I also evaluated both tuit17 and tuit18 seperately and found them not significant.




#### d) 

What is the correlation between tuit17 and tuit18? Explain why using the average of the tuition over the two years might be preferred to adding each separately. What happens when you do use the average? 



I'm still not able to discern statistical significance from the tuition data, which makes me think that I'm doing something wrong. Using the average doesn't change that, but might be preferred since the there may have been other factors influencing tuition in one year or another, like a scholarship, and so it doesn't make sense to evaluate them individually. Any motivation we might care about in one year will be present in the other. 





#### e) 

Do the findings for the average tuition variable in part iv make sense when interpreted causally? What might be going on?














