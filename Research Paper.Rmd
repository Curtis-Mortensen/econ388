---
title: "Research Paper"
author: "Max Krupczynski and Curtis Mortensen"
date: "19/05/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Author's Notes
http://mozzherin.org/2015/12/05/r-graphs-in-latex.html - for transfering this to overleaf. 

The data in question is too large to be loaded in Git. Some work-arounds are suggested when we try to push the commit. 

One illustration, 3 variables, summary stats, data limitations. 

##Data Analysis 
From the CPS Labor data, which contains about 30,000 interviews per month, and 50 variables, we selected the following variables:

intmonth, minsamp, state, fips, county, smsastat, centcity, hourslw, grade92

Interview month - intmonth
Month in sample - minsamp
State - "" 
State FIPS code - fips 
County FIPS code - county 
Metropolitan status code - smsastat 
Central city code - centcity
How many hours last all jobs
Earnings per hour - earnhre
 How many hours last week? - hourslw
Earnings per week - earnwke
  3-digit industry code
   Detailed industry code
   
Education
Highest grade completed - grade92



```{r}
library(haven)
library(tidyverse)
morg00 <- read_dta("morg00.dta")
View(morg00)

#vars <- select(morg00, ethnic, race)
#write.table(vars, file="~/econ388/vars.csv")
#we're gonna wanna sort the people who do and don't have earnhre

#incomplete_vars <- select(morg00, )

incomp <- morg00 %>% 
  filter(is.na(earnhre))

#incomp1 <- incomp[!is.na(incomp$hourslw), !is.na(incomp$earnwke)]
# I couldn't get this to work, but the below did

#or 

library(tidyr)
incomp2 <- morg00 %>% drop_na(earnwke, hourslw)

# ! = not
# k next we get to populate the NAs, and then bind them back. 
#185,958 values were NA, out of 283,236, and 154,533 of those had earnwke and hourslw. Let's verify it:

sum(is.na(incomp2$earnwke))
sum(is.na(incomp2$hourslw))

#verified. Now to populate, then merge back. 
#alternatively, I could just mutate based on an if statement to not have to remove and repop, and I also only want those rows that *do* have values for weekly

#incomp %>% mutate(earnhre = ifelse(column1 %in% 1:3, column2, column3))
```

The problem we encountered was that it is hard to extrapolate unemployment from this data. Also, if we sort the data by wage bins, our model would have to predict the effect on minimum wage on multiple different bins, that also affect each other as workers move between them. We're not sure we can capture that interaction in OLS. 

There's a couple of options we came up with:
1. If we assume that this was a representative sample we can possibly measure the amount of people who reported themselves as unemployed before and after a wage change, and extrapolate based on total state population how that may have changed the unemployment. We could also sort the people who did report being employed by wage bins, and see how the number of those bins changed, but we would have no guarentee that the same individuals were surveyed before and after. Since we don't know that, we wouldn't be able to be sure that some individuals didn't move states, or get much higher paying jobs, etc. 

Hypothetically, if we divided our population into 10 bins, say from $10 - $15, we could then report the increase or decrease in each wage bin as a data point. 

However, our model would have to have multiple dependent variables - each change in the wage bin would be a seperate y. 

2. Alternatively, we could limit our data to paired survey results, for the same individuals. We could then see what percentage of them had an increase in wage, and what percentage of them became unemployed, and we could limit that data further by low-wage workers. 

We could limit the data to individuals who were given the initial survey before the wage change, and then only individuals who were given the followup after the wage change. Even though we couldn't be sure they were the same individuals since they don't have identification numbers, we could be fairly certain we had captured the same population. 

The ideal would be to have a program, possibly a seris of if statements, that we give a state's FIPS number and the month a wage change occurred, and then it cleans the data for us, over 3 years, for all the paired individuals who were surveyed before and after. 

3. The last option we came up with would just be to extrapolate unemployment based on this representative sample. We could just multiply the percentage of the population surveyed by the total state population, and take as data points the change in the state unemployment from before to after the change. It would only give us one data point for each wage change, so our model might not be as robust, but it was what the other labor group seemed to be planning on doing. 
